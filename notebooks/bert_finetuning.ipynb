{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eea0a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /scratch/sagarsj42/torch-cache\n",
    "!mkdir -p /scratch/sagarsj42/transformers\n",
    "!mkdir -p /scratch/sagarsj42/hf-datasets\n",
    "\n",
    "import os\n",
    "os.chdir('/scratch/sagarsj42')\n",
    "os.environ['TORCH_HOME'] = '/scratch/sagarsj42/torch-cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/scratch/sagarsj42/transformers'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/scratch/sagarsj42/hf-datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fc4889f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-11 00:01:54.692087: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-11 00:01:54.692125: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import copy\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f28751e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.logging.set_verbosity_error()\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82c7c448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from explore_wikiqa.ipynb\n",
    "def get_valid_questions(wikiqa):\n",
    "    question_status = dict()\n",
    "\n",
    "    for split in wikiqa:\n",
    "        split_dataset = wikiqa[split]\n",
    "        n_samples = len(split_dataset)\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            qid = split_dataset[i]['question_id']\n",
    "            label = split_dataset[i]['label']\n",
    "            if qid not in question_status:\n",
    "                question_status[qid] = label\n",
    "            else:\n",
    "                question_status[qid] = max(question_status[qid], label)\n",
    "\n",
    "    valid_questions = set([qid for qid in question_status if question_status[qid] > 0])\n",
    "    \n",
    "    return valid_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c1b045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiqaDataset(Dataset):\n",
    "    def __init__(self, wikiqa, tokenizer, max_length):\n",
    "        super(WikiqaDataset, self).__init__()\n",
    "        self.wikiqa = wikiqa\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.wikiqa)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.wikiqa[idx]\n",
    "        question = sample['question'].translate(\n",
    "            str.maketrans('', '', string.punctuation)).lower().strip()\n",
    "        sentence = sample['answer'].translate(\n",
    "            str.maketrans('', '', string.punctuation)).lower().strip()\n",
    "        label = sample['label'] * 1.0\n",
    "        \n",
    "        input_enc = tokenizer(text=question, text_pair=sentence, \n",
    "                              add_special_tokens=True, truncation=True, padding='max_length', \n",
    "                              max_length=self.max_length, \n",
    "                              return_tensors='pt', return_attention_mask=True)\n",
    "        \n",
    "        return (sample['question_id'], question, sentence, \n",
    "                input_enc['input_ids'].flatten(), input_enc['attention_mask'].flatten(), \n",
    "                input_enc['token_type_ids'].flatten(), \n",
    "                label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff0e8e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertQA(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super(BertQA, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Dropout(p=0.15),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        a = x[1]\n",
    "        x = self.encoder(input_ids=x[0], attention_mask=x[1], token_type_ids=x[2]).last_hidden_state\n",
    "        x = (x * a.unsqueeze(-1) / a.sum(1).view(-1, 1, 1)).sum(1)\n",
    "        x = self.layers(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b884f20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home2/sagarsj42/.cache/huggingface/modules/datasets_modules/datasets/wiki_qa/d2d236b5cbdc6fbdab45d168b4d678a002e06ddea3525733a24558150585951c (last modified on Sat Nov 20 13:05:25 2021) since it couldn't be found locally at /scratch/sagarsj42/wiki_qa/wiki_qa.py, or remotely (ConnectionError).\n",
      "Using custom data configuration default\n",
      "Reusing dataset wiki_qa (/scratch/sagarsj42/hf-datasets/wiki_qa/default/0.1.0/d2d236b5cbdc6fbdab45d168b4d678a002e06ddea3525733a24558150585951c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa2713dfaf641d38bafd838b2da472d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163e42da5be74686a2225a3526ff36bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3699752fa864c9096eecdd58ebf92a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['question_id', 'question', 'document_title', 'answer', 'label'],\n",
       "        num_rows: 2351\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question_id', 'question', 'document_title', 'answer', 'label'],\n",
       "        num_rows: 1130\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['question_id', 'question', 'document_title', 'answer', 'label'],\n",
       "        num_rows: 8672\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikiqa = datasets.load_dataset('wiki_qa')\n",
    "valid_questions = get_valid_questions(wikiqa)\n",
    "wikiqa_f = wikiqa.filter(lambda sample: sample['question_id'] in valid_questions)\n",
    "\n",
    "wikiqa_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a9d670c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "encoder = BertModel.from_pretrained('bert-base-uncased')\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c806783d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8672"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_lengths = list()\n",
    "\n",
    "for sample in wikiqa_f['train']:\n",
    "    question = sample['question']\n",
    "    sentence = sample['answer']\n",
    "    tok_len = len(tokenizer(text=question, text_pair=sentence)['input_ids'])\n",
    "    tok_lengths.append(tok_len)\n",
    "    \n",
    "len(tok_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35e14889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(tok_lengths, density=False, bins=50)\n",
    "plt.xlabel('tokenized length')\n",
    "plt.ylabel('counts')\n",
    "plt.title('Histogram of tokenized question-sentence lengths')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab190090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCEWithLogitsLoss()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 5e-6\n",
    "n_epochs = 5\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3c0b926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8672, 1130, 2351)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = WikiqaDataset(wikiqa_f['train'], tokenizer, max_length=64)\n",
    "dev_dataset = WikiqaDataset(wikiqa_f['validation'], tokenizer, max_length=64)\n",
    "test_dataset = WikiqaDataset(wikiqa_f['test'], tokenizer, max_length=64)\n",
    "\n",
    "len(train_dataset), len(dev_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a362cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(542, 71, 147)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "len(train_dataloader), len(dev_dataloader), len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6511b40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, model, optimizer, criterion, device='cpu'):\n",
    "    model.train()\n",
    "    n_batches = len(dataloader)\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for sample in dataloader:\n",
    "        sample = sample[3:7]\n",
    "        sample = [s.to(device) for s in sample]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(sample[:-1])\n",
    "        loss = criterion(output.flatten(), sample[-1].flatten())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / n_batches\n",
    "\n",
    "def validate(dataloader, model, criterion, device='cpu'):\n",
    "    model.eval()\n",
    "    n_batches = len(dataloader)\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for sample in dataloader:\n",
    "        sample = sample[3:7]\n",
    "        sample = [s.to(device) for s in sample]\n",
    "        output = model(sample[:-1])\n",
    "        loss = criterion(output.flatten(), sample[-1].flatten())\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / n_batches\n",
    "\n",
    "def get_scores(dataloader, model, device='cpu'):\n",
    "    model.eval()\n",
    "    eval_results = list()\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        batch_d = [b.to(device) for b in batch[3:6]]\n",
    "        output = model(batch_d).detach()\n",
    "        scores = nn.Sigmoid()(output)\n",
    "        \n",
    "        batch[-1] = batch[-1].tolist()\n",
    "        batch.append(scores.flatten().tolist())\n",
    "        size = len(batch[0])\n",
    "        eval_results.extend([[b[i] for b in batch] for i in range(size)])\n",
    "        \n",
    "    return eval_results\n",
    "\n",
    "def get_question_label_scores(results):\n",
    "    q_dict = dict()\n",
    "    \n",
    "    for result in results:\n",
    "        qid = result[0]\n",
    "        if qid in q_dict:\n",
    "            q_dict[qid][1].append(result[2])\n",
    "            q_dict[qid][2].append(result[6])\n",
    "            q_dict[qid][3].append(result[7])\n",
    "        else:\n",
    "            q_dict[qid] = [result[1], [result[2]], [result[6]], [result[7]]]\n",
    "            \n",
    "    return q_dict\n",
    "\n",
    "def calculate_metrics(question_scores):\n",
    "    total = len(question_scores) * 1.0\n",
    "    thresholds = np.arange(10, 20) / 20\n",
    "    accuracy = 0\n",
    "    mrr = 0.0\n",
    "    mean_ap = dict()\n",
    "    \n",
    "    for qid, values in question_scores.items():\n",
    "        labels = values[2]\n",
    "        scores = values[3]\n",
    "        actual = np.array(labels).argmax()\n",
    "        predicted = np.array(scores).argmax()\n",
    "        expected_max = scores[actual]\n",
    "        scores.sort(reverse=True)\n",
    "        given_rank = scores.index(expected_max) + 1\n",
    "        \n",
    "        for t in thresholds:\n",
    "            tp = np.sum(scores[actual] > t)\n",
    "            tp_fp = np.sum(np.array(scores) > t)\n",
    "            p = 1.0 if tp_fp == 0 else tp/tp_fp\n",
    "            if t in mean_ap:\n",
    "                mean_ap[t] += p / total\n",
    "            else:\n",
    "                mean_ap[t] = 0.0\n",
    "        \n",
    "        if actual == predicted:\n",
    "            accuracy += 1\n",
    "        mrr += (1.0/given_rank)\n",
    "        \n",
    "    accuracy /= total\n",
    "    mean_ap = np.array(list(mean_ap.values())).mean()\n",
    "    mrr /= total\n",
    "    \n",
    "    return accuracy, mean_ap, mrr\n",
    "\n",
    "def plot_curves(train_stats, dev_stats, index, title):\n",
    "    n_epochs = len(train_stats)+1\n",
    "    plt.plot(range(1, n_epochs), [s[index] for s in train_stats])\n",
    "    plt.plot(range(1, n_epochs), [s[index] for s in dev_stats])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(title)\n",
    "    plt.title(f'Comparison of {title} variation over epochs')\n",
    "    plt.legend(['train', 'dev'])\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0c21980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-07\n",
       "    lr: 5e-06\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertQA(encoder)\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-7)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd75f271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 complete. Train loss: 0.3870, Dev loss: 0.2803\n",
      "Accuracy: train = 0.5154639175257731, dev = 0.6349206349206349\n",
      "MAP: train = 0.8979763268423033, dev = 0.9531746031746007\n",
      "MRR: train = 0.6920979247168427, dev = 0.745043118257404\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2 complete. Train loss: 0.3191, Dev loss: 0.2799\n",
      "Accuracy: train = 0.6242840778923253, dev = 0.6666666666666666\n",
      "MAP: train = 0.7584612447499021, dev = 0.8968253968253945\n",
      "MRR: train = 0.7746994700602952, dev = 0.7716773746535651\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3 complete. Train loss: 0.2831, Dev loss: 0.2754\n",
      "Accuracy: train = 0.7445589919816724, dev = 0.6428571428571429\n",
      "MAP: train = 0.6907082856051907, dev = 0.8665343915343893\n",
      "MRR: train = 0.8519432171494028, dev = 0.7678508440413201\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4 complete. Train loss: 0.2473, Dev loss: 0.2682\n",
      "Accuracy: train = 0.8339060710194731, dev = 0.6825396825396826\n",
      "MAP: train = 0.5394323078601413, dev = 0.8123015873015852\n",
      "MRR: train = 0.9049055850602242, dev = 0.7906588561350465\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5 complete. Train loss: 0.2135, Dev loss: 0.2991\n",
      "Accuracy: train = 0.8625429553264605, dev = 0.6349206349206349\n",
      "MAP: train = 0.41623452244586145, dev = 0.7274470899470883\n",
      "MRR: train = 0.9218849433626067, dev = 0.7658071646166884\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_mrr = float('-inf')\n",
    "best_model = None\n",
    "train_stats = list()\n",
    "dev_stats = list()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_epoch(train_dataloader, model, optimizer, criterion, device=DEVICE)\n",
    "    dev_loss = validate(dev_dataloader, model, criterion, device=DEVICE)\n",
    "    \n",
    "    train_results = get_scores(train_dataloader, model, device=DEVICE)\n",
    "    train_qscores = get_question_label_scores(train_results)\n",
    "    train_acc, train_map, train_mrr = calculate_metrics(train_qscores)\n",
    "    train_stats.append((train_loss, train_acc, train_map, train_mrr))\n",
    "    \n",
    "    dev_results = get_scores(dev_dataloader, model, device=DEVICE)\n",
    "    dev_qscores = get_question_label_scores(dev_results)\n",
    "    dev_acc, dev_map, dev_mrr = calculate_metrics(dev_qscores)\n",
    "    dev_stats.append((dev_loss, dev_acc, dev_map, dev_mrr))\n",
    "    \n",
    "    if dev_mrr > best_mrr:\n",
    "        best_mrr = dev_mrr\n",
    "        best_model = copy.deepcopy(model)\n",
    "    \n",
    "    print(f'Epoch {epoch+1} complete. Train loss: {train_loss:.4f}, Dev loss: {dev_loss:.4f}')\n",
    "    print(f'Accuracy: train = {train_acc}, dev = {dev_acc}')\n",
    "    print(f'MAP: train = {train_map}, dev = {dev_map}')\n",
    "    print(f'MRR: train = {train_mrr}, dev = {dev_mrr}')\n",
    "    print('-'*80)\n",
    "    \n",
    "save_dict = {'model_params': best_model.state_dict(), \n",
    "             'train_stats': train_stats, \n",
    "             'dev_stats': dev_stats\n",
    "            }\n",
    "torch.save(save_dict, 'best-wikiqa-bert.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20bc25b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model_params', 'train_stats', 'dev_stats'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dict = torch.load('best-wikiqa-bert.pth')\n",
    "train_stats = save_dict['train_stats']\n",
    "dev_stats = save_dict['dev_stats']\n",
    "save_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4eb5c977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertQA(\n",
       "  (encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "    (1): Dropout(p=0.25, inplace=False)\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "    (3): Linear(in_features=512, out_features=1, bias=True)\n",
       "    (4): Dropout(p=0.15, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertQA(encoder)\n",
    "model.load_state_dict(save_dict['model_params'])\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f787216",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 10.76 GiB total capacity; 3.57 GiB already allocated; 8.12 MiB free; 3.78 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_33686/4172504696.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_33686/3194826545.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(dataloader, model, criterion, device)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_33686/864703533.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    994\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m         )\n\u001b[0;32m--> 996\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    997\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    581\u001b[0m                 )\n\u001b[1;32m    582\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    584\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    512\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         )\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   2347\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2349\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mgelu\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m   1553\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_unary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1555\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 10.76 GiB total capacity; 3.57 GiB already allocated; 8.12 MiB free; 3.78 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "test_loss = validate(test_dataloader, model, criterion, device=DEVICE)\n",
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75222382",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = get_scores(test_dataloader, model, device=DEVICE)\n",
    "test_qscores = get_question_label_scores(test_results)\n",
    "test_acc, test_map, test_mrr = calculate_metrics(test_qscores)\n",
    "\n",
    "print(f'Test data: Accuracy = {test_acc}, MAP = {test_map}, MRR = {test_mrr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a36ca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(train_stats, dev_stats, 0, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1377d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(train_stats, dev_stats, 1, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7692e5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(train_stats, dev_stats, 2, 'MAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41868e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(train_stats, dev_stats, 3, 'MRR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b626577b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
