{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "361defd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /scratch/sagarsj42/torch-cache\n",
    "!mkdir -p /scratch/sagarsj42/transformers\n",
    "!mkdir -p /scratch/sagarsj42/hf-datasets\n",
    "\n",
    "import os\n",
    "os.chdir('/scratch/sagarsj42')\n",
    "os.environ['TORCH_HOME'] = '/scratch/sagarsj42/torch-cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/scratch/sagarsj42/transformers'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/scratch/sagarsj42/hf-datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "700f0ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import string\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import datasets\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70e8c609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WikiQACorpus.zip                              100% 6928KB   6.8MB/s   00:00    \n",
      "Archive:  WikiQACorpus.zip\n",
      "   creating: WikiQACorpus/emnlp-table/\n",
      "  inflating: WikiQACorpus/emnlp-table/WikiQA.CNN.dev.rank  \n",
      "  inflating: WikiQACorpus/emnlp-table/WikiQA.CNN.test.rank  \n",
      "  inflating: WikiQACorpus/emnlp-table/WikiQA.CNN-Cnt.dev.rank  \n",
      "  inflating: WikiQACorpus/emnlp-table/WikiQA.CNN-Cnt.test.rank  \n",
      "  inflating: WikiQACorpus/eval.py    \n",
      "  inflating: WikiQACorpus/Guidelines_Phase1.pdf  \n",
      "  inflating: WikiQACorpus/Guidelines_Phase2.pdf  \n",
      "  inflating: WikiQACorpus/WikiQA.tsv  \n",
      "  inflating: WikiQACorpus/WikiQA-dev.ref  \n",
      "  inflating: WikiQACorpus/WikiQA-dev.tsv  \n",
      "  inflating: WikiQACorpus/WikiQA-dev.txt  \n",
      "  inflating: WikiQACorpus/WikiQA-dev-filtered.ref  \n",
      "  inflating: WikiQACorpus/WikiQASent.pos.ans.tsv  \n",
      "  inflating: WikiQACorpus/WikiQA-test.ref  \n",
      "  inflating: WikiQACorpus/WikiQA-test.tsv  \n",
      "  inflating: WikiQACorpus/WikiQA-test.txt  \n",
      "  inflating: WikiQACorpus/WikiQA-test-filtered.ref  \n",
      "  inflating: WikiQACorpus/WikiQA-train.ref  \n",
      "  inflating: WikiQACorpus/WikiQA-train.tsv  \n",
      "  inflating: WikiQACorpus/WikiQA-train.txt  \n",
      "  inflating: WikiQACorpus/LICENSE.pdf  \n",
      "  inflating: WikiQACorpus/README.txt  \n"
     ]
    }
   ],
   "source": [
    "# !scp sagarsj42@ada:/share1/sagarsj42/WikiQACorpus.zip .\n",
    "# !unzip -o WikiQACorpus.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ced921c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-12-09 14:33:27--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2021-12-09 14:33:28--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  4.93MB/s    in 4m 34s  \n",
      "\n",
      "2021-12-09 14:38:03 (3.00 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n",
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    }
   ],
   "source": [
    "# !wget -c https://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip -o glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7adfe38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71026954",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_FILE = 'glove.6B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "518e8e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from explore_wikiqa.ipynb\n",
    "\n",
    "def get_valid_questions(wikiqa):\n",
    "    question_status = dict()\n",
    "\n",
    "    for split in wikiqa:\n",
    "        split_dataset = wikiqa[split]\n",
    "        n_samples = len(split_dataset)\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            qid = split_dataset[i]['question_id']\n",
    "            label = split_dataset[i]['label']\n",
    "            if qid not in question_status:\n",
    "                question_status[qid] = label\n",
    "            else:\n",
    "                question_status[qid] = max(question_status[qid], label)\n",
    "\n",
    "    valid_questions = set([qid for qid in question_status if question_status[qid] > 0])\n",
    "    \n",
    "    return valid_questions\n",
    "\n",
    "\n",
    "def load_glove(filename):\n",
    "    glove = dict()\n",
    "\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line_content = line.split()\n",
    "            word = line_content[0].strip()\n",
    "            vec = np.array(line_content[1:], dtype='float32')\n",
    "            glove[word] = vec\n",
    "            \n",
    "    return glove\n",
    "\n",
    "\n",
    "def get_tokens(sample):\n",
    "    question = sample['question'].translate(str.maketrans('', '', string.punctuation))\n",
    "    question = question.lower().split()\n",
    "    \n",
    "    answer = sample['answer'].translate(str.maketrans('', '', string.punctuation))\n",
    "    answer = answer.lower().split()\n",
    "    \n",
    "    return question, answer\n",
    "\n",
    "\n",
    "def get_embeddings(q_a_tokens, glove):\n",
    "    embed_size = len(list(glove.values())[0])\n",
    "    q_vecs = [glove[q_word] if q_word in glove else np.zeros(embed_size) for q_word in q_a_tokens[0]]\n",
    "    a_vecs = [glove[a_word] if a_word in glove else np.zeros(embed_size) for a_word in q_a_tokens[1]]\n",
    "    \n",
    "    return q_vecs, a_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0eed563f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home2/sagarsj42/.cache/huggingface/modules/datasets_modules/datasets/wiki_qa/d2d236b5cbdc6fbdab45d168b4d678a002e06ddea3525733a24558150585951c (last modified on Sat Nov 20 13:05:25 2021) since it couldn't be found locally at /scratch/sagarsj42/wiki_qa/wiki_qa.py, or remotely (ConnectionError).\n",
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wiki_qa/default (download: 6.77 MiB, generated: 6.10 MiB, post-processed: Unknown size, total: 12.87 MiB) to /scratch/sagarsj42/hf-datasets/wiki_qa/default/0.1.0/d2d236b5cbdc6fbdab45d168b4d678a002e06ddea3525733a24558150585951c...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70c0a4c469784a84af9081ed20babd55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/7.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset wiki_qa downloaded and prepared to /scratch/sagarsj42/hf-datasets/wiki_qa/default/0.1.0/d2d236b5cbdc6fbdab45d168b4d678a002e06ddea3525733a24558150585951c. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556482e1f72846439d350a3f7ccdc776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a772241bdc4b4e9f11aeb3ff466443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f510a826cf0045448e07969757bc47b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['question_id', 'question', 'document_title', 'answer', 'label'],\n",
       "        num_rows: 2351\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question_id', 'question', 'document_title', 'answer', 'label'],\n",
       "        num_rows: 1130\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['question_id', 'question', 'document_title', 'answer', 'label'],\n",
       "        num_rows: 8672\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikiqa = datasets.load_dataset('wiki_qa')\n",
    "valid_questions = get_valid_questions(wikiqa)\n",
    "wikiqa_f = wikiqa.filter(lambda sample: sample['question_id'] in valid_questions)\n",
    "\n",
    "wikiqa_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6422339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove = load_glove(GLOVE_FILE)\n",
    "\n",
    "len(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "080000f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiqaDataset(Dataset):\n",
    "    def __init__(self, wikiqa, glove):\n",
    "        super(WikiqaDataset, self).__init__()\n",
    "        self.wikiqa = wikiqa\n",
    "        self.glove = glove\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.wikiqa)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.wikiqa[idx]\n",
    "        question, sentence = get_embeddings(get_tokens(sample), self.glove)\n",
    "        label = torch.tensor([sample['label']], dtype=torch.long)\n",
    "        \n",
    "        question = torch.cat([torch.Tensor(q_word).view(1, -1) for q_word in question], dim=0)\n",
    "        sentence = torch.cat([torch.Tensor(s_word).view(1, -1) for s_word in sentence], dim=0)\n",
    "        \n",
    "        return question, sentence, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "677aa965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dual_att_projection',\n",
       "  Parameter containing:\n",
       "  tensor([[0.1529, 0.8791, 0.3513,  ..., 0.8848, 0.2281, 0.5123],\n",
       "          [0.7468, 0.2274, 0.4268,  ..., 0.3958, 0.6680, 0.7066],\n",
       "          [0.2946, 0.5287, 0.7872,  ..., 0.3011, 0.0245, 0.5880],\n",
       "          ...,\n",
       "          [0.2874, 0.9593, 0.3186,  ..., 0.1475, 0.9392, 0.3406],\n",
       "          [0.2208, 0.8260, 0.2532,  ..., 0.4973, 0.4104, 0.6009],\n",
       "          [0.2112, 0.1792, 0.7435,  ..., 0.5697, 0.5224, 0.5755]],\n",
       "         requires_grad=True)),\n",
       " ('projection.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0349,  0.0487, -0.0144,  ...,  0.0489, -0.0456, -0.0159],\n",
       "          [-0.0455,  0.0402, -0.0339,  ...,  0.0475, -0.0372,  0.0014],\n",
       "          [ 0.0435, -0.0350, -0.0334,  ...,  0.0576,  0.0382,  0.0255],\n",
       "          ...,\n",
       "          [ 0.0461,  0.0570,  0.0330,  ..., -0.0333, -0.0195,  0.0419],\n",
       "          [ 0.0444,  0.0317, -0.0537,  ..., -0.0456,  0.0317,  0.0229],\n",
       "          [ 0.0407,  0.0024,  0.0091,  ...,  0.0171,  0.0161,  0.0456]],\n",
       "         requires_grad=True)),\n",
       " ('projection.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0101,  0.0294, -0.0006,  0.0194,  0.0547,  0.0360, -0.0182, -0.0233,\n",
       "           0.0147, -0.0158,  0.0188,  0.0522, -0.0241, -0.0504,  0.0554,  0.0284,\n",
       "           0.0537,  0.0260, -0.0187, -0.0162, -0.0080,  0.0176, -0.0485,  0.0516,\n",
       "           0.0209,  0.0539,  0.0198,  0.0526, -0.0540,  0.0480,  0.0569, -0.0490,\n",
       "          -0.0108, -0.0400, -0.0569,  0.0164,  0.0352, -0.0429, -0.0030,  0.0143,\n",
       "          -0.0027,  0.0351, -0.0296,  0.0024,  0.0486,  0.0254, -0.0179,  0.0251,\n",
       "          -0.0306,  0.0445,  0.0044, -0.0518, -0.0128, -0.0193,  0.0220,  0.0394,\n",
       "          -0.0570,  0.0037,  0.0208,  0.0403,  0.0200,  0.0097, -0.0277, -0.0238,\n",
       "           0.0309,  0.0455,  0.0165,  0.0568, -0.0419,  0.0436, -0.0032, -0.0313,\n",
       "          -0.0377, -0.0499, -0.0022,  0.0438,  0.0395,  0.0326, -0.0265,  0.0346,\n",
       "          -0.0128,  0.0344, -0.0549,  0.0516,  0.0327, -0.0284,  0.0002, -0.0097,\n",
       "          -0.0550, -0.0032, -0.0488,  0.0068,  0.0248, -0.0459,  0.0513,  0.0077,\n",
       "          -0.0462,  0.0392, -0.0543, -0.0046, -0.0166, -0.0515, -0.0027,  0.0042,\n",
       "           0.0312, -0.0060,  0.0573, -0.0474,  0.0286,  0.0450,  0.0297, -0.0391,\n",
       "           0.0503, -0.0074,  0.0357, -0.0525, -0.0189,  0.0077,  0.0375, -0.0492,\n",
       "           0.0132, -0.0160, -0.0247,  0.0518, -0.0082, -0.0095,  0.0444,  0.0204,\n",
       "           0.0357, -0.0541,  0.0141,  0.0539,  0.0262, -0.0097,  0.0021, -0.0294,\n",
       "           0.0238,  0.0398, -0.0162, -0.0369, -0.0067, -0.0385, -0.0057, -0.0194,\n",
       "           0.0348,  0.0024, -0.0095, -0.0071, -0.0041,  0.0461,  0.0327,  0.0235,\n",
       "           0.0342,  0.0352, -0.0487,  0.0259,  0.0256,  0.0176, -0.0483,  0.0404,\n",
       "           0.0398,  0.0193, -0.0118, -0.0271,  0.0530,  0.0517, -0.0354, -0.0524,\n",
       "          -0.0260, -0.0554, -0.0174, -0.0038,  0.0371, -0.0470, -0.0399, -0.0167,\n",
       "           0.0452,  0.0163,  0.0524,  0.0113, -0.0173, -0.0413,  0.0439, -0.0066,\n",
       "          -0.0293,  0.0043,  0.0539, -0.0310, -0.0484,  0.0484, -0.0115, -0.0340,\n",
       "          -0.0465,  0.0430, -0.0375,  0.0134,  0.0072,  0.0039,  0.0418,  0.0381,\n",
       "           0.0433, -0.0343,  0.0232,  0.0498, -0.0504, -0.0387, -0.0050, -0.0450,\n",
       "           0.0341, -0.0506,  0.0416, -0.0277,  0.0258,  0.0351, -0.0018, -0.0229,\n",
       "          -0.0569,  0.0510, -0.0008, -0.0449, -0.0251,  0.0246, -0.0345,  0.0192,\n",
       "           0.0233, -0.0251, -0.0106,  0.0186,  0.0209,  0.0397, -0.0142,  0.0466,\n",
       "          -0.0318,  0.0525,  0.0154, -0.0290,  0.0475,  0.0286, -0.0230, -0.0107,\n",
       "           0.0349,  0.0414,  0.0031, -0.0356, -0.0530, -0.0069,  0.0189, -0.0187,\n",
       "          -0.0318,  0.0361, -0.0323,  0.0199, -0.0098,  0.0371,  0.0518,  0.0418,\n",
       "          -0.0393,  0.0573, -0.0363,  0.0513, -0.0011, -0.0299, -0.0261, -0.0217,\n",
       "           0.0072,  0.0309,  0.0309,  0.0065,  0.0422, -0.0074, -0.0099,  0.0081,\n",
       "          -0.0328, -0.0248, -0.0198,  0.0101, -0.0355, -0.0319,  0.0228, -0.0566,\n",
       "           0.0398,  0.0358,  0.0348,  0.0231,  0.0471,  0.0419, -0.0255, -0.0428,\n",
       "           0.0484, -0.0074, -0.0144, -0.0231,  0.0324, -0.0020, -0.0343,  0.0229,\n",
       "          -0.0569, -0.0329,  0.0247,  0.0080], requires_grad=True)),\n",
       " ('lstm.weight_ih_l0',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0132,  0.0354,  0.0412,  ...,  0.0426, -0.0461,  0.0420],\n",
       "          [-0.0373,  0.0468,  0.0298,  ..., -0.0050,  0.0018, -0.0180],\n",
       "          [ 0.0573, -0.0378, -0.0571,  ..., -0.0251,  0.0383,  0.0158],\n",
       "          ...,\n",
       "          [-0.0252,  0.0351,  0.0054,  ..., -0.0507, -0.0132, -0.0376],\n",
       "          [-0.0436, -0.0157,  0.0028,  ...,  0.0485,  0.0428, -0.0173],\n",
       "          [ 0.0484, -0.0569,  0.0015,  ...,  0.0467, -0.0516, -0.0557]],\n",
       "         requires_grad=True)),\n",
       " ('lstm.weight_hh_l0',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0134,  0.0545,  0.0563,  ...,  0.0232, -0.0107,  0.0338],\n",
       "          [ 0.0374,  0.0567,  0.0055,  ..., -0.0550, -0.0042,  0.0425],\n",
       "          [-0.0495,  0.0266,  0.0569,  ...,  0.0165,  0.0300,  0.0105],\n",
       "          ...,\n",
       "          [ 0.0398,  0.0317,  0.0459,  ..., -0.0417, -0.0328,  0.0108],\n",
       "          [ 0.0214, -0.0024, -0.0404,  ...,  0.0272,  0.0168, -0.0269],\n",
       "          [-0.0542, -0.0243, -0.0477,  ...,  0.0455,  0.0240, -0.0074]],\n",
       "         requires_grad=True)),\n",
       " ('lstm.bias_ih_l0',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0376, -0.0509, -0.0327,  ..., -0.0544, -0.0424,  0.0329],\n",
       "         requires_grad=True)),\n",
       " ('lstm.bias_hh_l0',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0196,  0.0345,  0.0573,  ...,  0.0513,  0.0420, -0.0515],\n",
       "         requires_grad=True)),\n",
       " ('lstm.weight_ih_l0_reverse',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0079,  0.0530, -0.0447,  ...,  0.0310, -0.0354, -0.0532],\n",
       "          [-0.0527,  0.0110, -0.0456,  ..., -0.0186,  0.0196,  0.0533],\n",
       "          [ 0.0196, -0.0577, -0.0416,  ..., -0.0308,  0.0182,  0.0177],\n",
       "          ...,\n",
       "          [-0.0399,  0.0109,  0.0268,  ...,  0.0516, -0.0354, -0.0301],\n",
       "          [-0.0111,  0.0158,  0.0197,  ..., -0.0262,  0.0329,  0.0367],\n",
       "          [ 0.0042,  0.0391,  0.0323,  ...,  0.0317,  0.0461,  0.0085]],\n",
       "         requires_grad=True)),\n",
       " ('lstm.weight_hh_l0_reverse',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0071,  0.0423, -0.0499,  ...,  0.0515, -0.0154, -0.0154],\n",
       "          [-0.0428,  0.0280, -0.0237,  ..., -0.0577,  0.0361,  0.0410],\n",
       "          [-0.0214, -0.0375, -0.0522,  ...,  0.0105, -0.0475, -0.0046],\n",
       "          ...,\n",
       "          [ 0.0016,  0.0391, -0.0522,  ...,  0.0466,  0.0352,  0.0111],\n",
       "          [ 0.0462, -0.0440,  0.0075,  ..., -0.0219, -0.0389,  0.0335],\n",
       "          [ 0.0562, -0.0576,  0.0069,  ...,  0.0064,  0.0131,  0.0553]],\n",
       "         requires_grad=True)),\n",
       " ('lstm.bias_ih_l0_reverse',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0170, -0.0056,  0.0270,  ..., -0.0091,  0.0554,  0.0349],\n",
       "         requires_grad=True)),\n",
       " ('lstm.bias_hh_l0_reverse',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0171,  0.0390, -0.0148,  ..., -0.0008, -0.0483,  0.0137],\n",
       "         requires_grad=True))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AttPoolLSTM(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, bidirectional=True):\n",
    "        super(AttPoolLSTM, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.projection = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=self.embed_dim, hidden_size=self.hidden_dim, num_layers=1, \n",
    "                            batch_first=True, bidirectional=bidirectional)\n",
    "        \n",
    "        self.dual_att_projection = nn.Parameter(torch.rand(2*self.hidden_dim, 2*self.hidden_dim))\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        self.cosine = nn.CosineSimilarity(dim=-1)\n",
    "        \n",
    "        \n",
    "    def forward(self, question, sentence):\n",
    "        if len(question.shape) == 2:\n",
    "            question = question.unsqueeze(dim=0)\n",
    "            sentence = sentence.unsqueeze(dim=0)\n",
    "        \n",
    "        question = self.projection(question)\n",
    "        question, _ = self.lstm(question)\n",
    "        sentence = self.projection(sentence)\n",
    "        sentence, _ = self.lstm(sentence)\n",
    "        \n",
    "        n = sentence.shape[0]\n",
    "        l = sentence.shape[1]\n",
    "        c = sentence.shape[2]\n",
    "        qs_alignment = torch.matmul(torch.matmul(question, self.dual_att_projection), sentence.view(n, c, l))\n",
    "        \n",
    "        q_pool = torch.max(qs_alignment, dim=2)[0]\n",
    "        q_pool = self.softmax(q_pool)\n",
    "        s_pool = torch.max(qs_alignment, dim=1)[0]\n",
    "        s_pool = self.softmax(s_pool)\n",
    "        \n",
    "        q_rep = torch.matmul(question.transpose(1, 2), q_pool.transpose(0, 1)).transpose(1, 2)\n",
    "        s_rep = torch.matmul(sentence.transpose(1, 2), s_pool.transpose(0, 1)).transpose(1, 2)\n",
    "        \n",
    "        match_score = self.cosine(q_rep, s_rep)\n",
    "        \n",
    "        return match_score\n",
    "    \n",
    "\n",
    "model = AttPoolLSTM(300, 300)\n",
    "list(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9758abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6, 300]), torch.Size([1, 35, 300]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question, sentence = get_embeddings(get_tokens(wikiqa_f['train'][100]), glove)\n",
    "question = torch.cat([torch.Tensor(q_word).view(1, -1) for q_word in question], dim=0).unsqueeze(0)\n",
    "sentence = torch.cat([torch.Tensor(s_word).view(1, -1) for s_word in sentence], dim=0).unsqueeze(0)\n",
    "\n",
    "question.shape, sentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c332ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8153]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_score = model(question, sentence)\n",
    "match_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6eab7d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttPoolLSTM(\n",
       "  (projection): Linear(in_features=300, out_features=300, bias=True)\n",
       "  (lstm): LSTM(300, 400, batch_first=True, bidirectional=True)\n",
       "  (softmax): Softmax(dim=-1)\n",
       "  (cosine): CosineSimilarity()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AttPoolLSTM(embed_dim=300, hidden_dim=400)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c442923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8672, 1130, 2351)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = WikiqaDataset(wikiqa_f['train'], glove)\n",
    "dev_dataset = WikiqaDataset(wikiqa_f['validation'], glove)\n",
    "test_dataset = WikiqaDataset(wikiqa_f['test'], glove)\n",
    "\n",
    "len(train_dataset), len(dev_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ad3ad9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarginRankingLoss()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 4\n",
    "n_epochs = 20\n",
    "\n",
    "criterion = nn.MarginRankingLoss(margin=0.1)\n",
    "criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1e09382f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0.0\n",
       "    lr: 0.001\n",
       "    momentum: 0.6\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0001\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.6, weight_decay=0.0001, \n",
    "                      dampening=0.0, nesterov=False)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4d0e0219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_candidate(dataset, indices, model, select_from=20, device='cpu'):\n",
    "    candidates = random.sample(indices, select_from)\n",
    "    candidate_scores = list()\n",
    "    for c_i in candidates:\n",
    "        c_q, c_s, c_l = dataset[c_i]\n",
    "        c_q = c_q.to(device)\n",
    "        c_s = c_s.to(device)\n",
    "        \n",
    "        if c_l == 0:\n",
    "            cand_match_score = model(c_q, c_s)\n",
    "            candidate_scores.append(cand_match_score.item())\n",
    "    if len(candidate_scores) > 0:\n",
    "        neg_sample_idx = candidates[np.argmax(np.array(candidate_scores))]\n",
    "    else:\n",
    "        neg_sample_idx = 0\n",
    "        \n",
    "    neg_sample = dataset[neg_sample_idx]\n",
    "    n_question, n_sentence, _ = neg_sample\n",
    "    \n",
    "    return n_question.to(device), n_sentence.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8852d8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataset, model, optimizer, criterion, batch_size, device='cpu'):\n",
    "    model.train()\n",
    "    indices = list(range(len(dataset)))\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    step_loss = 0.0\n",
    "    batch_loss = 0.0\n",
    "    batch_count = 0\n",
    "    step_count = 0\n",
    "    \n",
    "    for i in indices:\n",
    "        question, sentence, label = dataset[i]\n",
    "        if label == 1:\n",
    "            question = question.to(device)\n",
    "            sentence = sentence.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            pos_match_score = model(question, sentence)\n",
    "            n_question, n_sentence = find_best_candidate(dataset, indices, model, \n",
    "                                                         select_from=20, device=device)\n",
    "            neg_match_score = model(n_question, n_sentence)\n",
    "            loss = criterion(pos_match_score, neg_match_score, label)\n",
    "            batch_loss += loss\n",
    "            batch_count += 1\n",
    "            \n",
    "            if batch_count % batch_size == 0:\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                batch_loss = 0.0\n",
    "                batch_count = 0\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            step_loss += loss.item()\n",
    "            step_count += 1\n",
    "            \n",
    "            if step_count % 100 == 0:\n",
    "                print(f'Step {step_count} | step loss: {step_loss/100.0:.4f}')\n",
    "                step_loss = 0.0\n",
    "            \n",
    "    return total_loss / step_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7fdfea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataset, model, device='cpu'):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    step_count = 0\n",
    "    indices = list(range(len(dataset)))\n",
    "    \n",
    "    for i in indices:\n",
    "        question, sentence, label = dataset[i]\n",
    "        if label == 1:\n",
    "            question = question.to(device)\n",
    "            sentence = sentence.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pos_match_score = model(question, sentence)\n",
    "            n_question, n_sentence = find_best_candidate(dataset, indices, model, \n",
    "                                                         select_from=20, device=device)\n",
    "            neg_match_score = model(n_question, n_sentence)\n",
    "            loss = criterion(pos_match_score, neg_match_score, label)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            step_count += 1\n",
    "    \n",
    "    return total_loss / step_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "95fac3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100 | step loss: 0.0953\n",
      "Step 200 | step loss: 0.0914\n",
      "Step 300 | step loss: 0.0957\n",
      "Step 400 | step loss: 0.0997\n",
      "Step 500 | step loss: 0.1018\n",
      "Step 600 | step loss: 0.1042\n",
      "Step 700 | step loss: 0.0956\n",
      "Step 800 | step loss: 0.0964\n",
      "Step 900 | step loss: 0.1037\n",
      "Step 1000 | step loss: 0.0959\n",
      "Epoch 1 complete.\n",
      "Train loss:0.0977, Dev loss:0.1135\n",
      "Step 100 | step loss: 0.0946\n",
      "Step 200 | step loss: 0.0827\n",
      "Step 300 | step loss: 0.0978\n",
      "Step 400 | step loss: 0.0915\n",
      "Step 500 | step loss: 0.0846\n",
      "Step 600 | step loss: 0.0972\n",
      "Step 700 | step loss: 0.0931\n",
      "Step 800 | step loss: 0.1047\n",
      "Step 900 | step loss: 0.0970\n",
      "Step 1000 | step loss: 0.0969\n",
      "Epoch 2 complete.\n",
      "Train loss:0.0945, Dev loss:0.1205\n",
      "Step 100 | step loss: 0.0878\n",
      "Step 200 | step loss: 0.0796\n",
      "Step 300 | step loss: 0.0924\n",
      "Step 400 | step loss: 0.0972\n",
      "Step 500 | step loss: 0.0990\n",
      "Step 600 | step loss: 0.0988\n",
      "Step 700 | step loss: 0.0972\n",
      "Step 800 | step loss: 0.1029\n",
      "Step 900 | step loss: 0.0998\n",
      "Step 1000 | step loss: 0.0863\n",
      "Epoch 3 complete.\n",
      "Train loss:0.0942, Dev loss:0.1180\n",
      "Step 100 | step loss: 0.0849\n",
      "Step 200 | step loss: 0.0864\n",
      "Step 300 | step loss: 0.0812\n",
      "Step 400 | step loss: 0.0903\n",
      "Step 500 | step loss: 0.0975\n",
      "Step 600 | step loss: 0.0950\n",
      "Step 700 | step loss: 0.0925\n",
      "Step 800 | step loss: 0.0960\n",
      "Step 900 | step loss: 0.0891\n",
      "Step 1000 | step loss: 0.0839\n",
      "Epoch 4 complete.\n",
      "Train loss:0.0893, Dev loss:0.1173\n",
      "Step 100 | step loss: 0.0848\n",
      "Step 200 | step loss: 0.0892\n",
      "Step 300 | step loss: 0.0895\n",
      "Step 400 | step loss: 0.0931\n",
      "Step 500 | step loss: 0.0884\n",
      "Step 600 | step loss: 0.0920\n",
      "Step 700 | step loss: 0.0839\n",
      "Step 800 | step loss: 0.0843\n",
      "Step 900 | step loss: 0.0887\n",
      "Step 1000 | step loss: 0.0719\n",
      "Epoch 5 complete.\n",
      "Train loss:0.0872, Dev loss:0.1212\n",
      "Step 100 | step loss: 0.0836\n",
      "Step 200 | step loss: 0.0881\n",
      "Step 300 | step loss: 0.0844\n",
      "Step 400 | step loss: 0.0798\n",
      "Step 500 | step loss: 0.0789\n",
      "Step 600 | step loss: 0.0865\n",
      "Step 700 | step loss: 0.0859\n",
      "Step 800 | step loss: 0.0912\n",
      "Step 900 | step loss: 0.0876\n",
      "Step 1000 | step loss: 0.0868\n",
      "Epoch 6 complete.\n",
      "Train loss:0.0854, Dev loss:0.1130\n",
      "Step 100 | step loss: 0.0809\n",
      "Step 200 | step loss: 0.0912\n",
      "Step 300 | step loss: 0.0687\n",
      "Step 400 | step loss: 0.0839\n",
      "Step 500 | step loss: 0.0745\n",
      "Step 600 | step loss: 0.0819\n",
      "Step 700 | step loss: 0.0865\n",
      "Step 800 | step loss: 0.0742\n",
      "Step 900 | step loss: 0.0839\n",
      "Step 1000 | step loss: 0.0861\n",
      "Epoch 7 complete.\n",
      "Train loss:0.0814, Dev loss:0.1086\n",
      "Step 100 | step loss: 0.0895\n",
      "Step 200 | step loss: 0.0767\n",
      "Step 300 | step loss: 0.0851\n",
      "Step 400 | step loss: 0.0790\n",
      "Step 500 | step loss: 0.0767\n",
      "Step 600 | step loss: 0.0717\n",
      "Step 700 | step loss: 0.0889\n",
      "Step 800 | step loss: 0.0792\n",
      "Step 900 | step loss: 0.0856\n",
      "Step 1000 | step loss: 0.0813\n",
      "Epoch 8 complete.\n",
      "Train loss:0.0811, Dev loss:0.1122\n",
      "Step 100 | step loss: 0.0822\n",
      "Step 200 | step loss: 0.0791\n",
      "Step 300 | step loss: 0.0885\n",
      "Step 400 | step loss: 0.0887\n",
      "Step 500 | step loss: 0.0836\n",
      "Step 600 | step loss: 0.0884\n",
      "Step 700 | step loss: 0.0829\n",
      "Step 800 | step loss: 0.0806\n",
      "Step 900 | step loss: 0.0821\n",
      "Step 1000 | step loss: 0.0780\n",
      "Epoch 9 complete.\n",
      "Train loss:0.0835, Dev loss:0.1109\n",
      "Step 100 | step loss: 0.0861\n",
      "Step 200 | step loss: 0.0748\n",
      "Step 300 | step loss: 0.0733\n",
      "Step 400 | step loss: 0.0809\n",
      "Step 500 | step loss: 0.0803\n",
      "Step 600 | step loss: 0.0751\n",
      "Step 700 | step loss: 0.0658\n",
      "Step 800 | step loss: 0.0891\n",
      "Step 900 | step loss: 0.0802\n",
      "Step 1000 | step loss: 0.0737\n",
      "Epoch 10 complete.\n",
      "Train loss:0.0783, Dev loss:0.1265\n",
      "Step 100 | step loss: 0.0818\n",
      "Step 200 | step loss: 0.0699\n",
      "Step 300 | step loss: 0.0767\n",
      "Step 400 | step loss: 0.0793\n",
      "Step 500 | step loss: 0.0743\n",
      "Step 600 | step loss: 0.0743\n",
      "Step 700 | step loss: 0.0754\n",
      "Step 800 | step loss: 0.0787\n",
      "Step 900 | step loss: 0.0840\n",
      "Step 1000 | step loss: 0.0763\n",
      "Epoch 11 complete.\n",
      "Train loss:0.0766, Dev loss:0.1236\n",
      "Step 100 | step loss: 0.0779\n",
      "Step 200 | step loss: 0.0780\n",
      "Step 300 | step loss: 0.0736\n",
      "Step 400 | step loss: 0.0681\n",
      "Step 500 | step loss: 0.0729\n",
      "Step 600 | step loss: 0.0705\n",
      "Step 700 | step loss: 0.0749\n",
      "Step 800 | step loss: 0.0713\n",
      "Step 900 | step loss: 0.0735\n",
      "Step 1000 | step loss: 0.0731\n",
      "Epoch 12 complete.\n",
      "Train loss:0.0740, Dev loss:0.1209\n",
      "Step 100 | step loss: 0.0609\n",
      "Step 200 | step loss: 0.0735\n",
      "Step 300 | step loss: 0.0648\n",
      "Step 400 | step loss: 0.0836\n",
      "Step 500 | step loss: 0.0840\n",
      "Step 600 | step loss: 0.0770\n",
      "Step 700 | step loss: 0.0738\n",
      "Step 800 | step loss: 0.0852\n",
      "Step 900 | step loss: 0.0636\n",
      "Step 1000 | step loss: 0.0808\n",
      "Epoch 13 complete.\n",
      "Train loss:0.0749, Dev loss:0.1238\n",
      "Step 100 | step loss: 0.0788\n",
      "Step 200 | step loss: 0.0739\n",
      "Step 300 | step loss: 0.0791\n",
      "Step 400 | step loss: 0.0687\n",
      "Step 500 | step loss: 0.0789\n",
      "Step 600 | step loss: 0.0787\n",
      "Step 700 | step loss: 0.0687\n",
      "Step 800 | step loss: 0.0771\n",
      "Step 900 | step loss: 0.0837\n",
      "Step 1000 | step loss: 0.0931\n",
      "Epoch 14 complete.\n",
      "Train loss:0.0781, Dev loss:0.1128\n",
      "Step 100 | step loss: 0.0785\n",
      "Step 200 | step loss: 0.0883\n",
      "Step 300 | step loss: 0.0798\n",
      "Step 400 | step loss: 0.0534\n",
      "Step 500 | step loss: 0.0807\n",
      "Step 600 | step loss: 0.0660\n",
      "Step 700 | step loss: 0.0828\n",
      "Step 800 | step loss: 0.0766\n",
      "Step 900 | step loss: 0.0759\n",
      "Step 1000 | step loss: 0.0780\n",
      "Epoch 15 complete.\n",
      "Train loss:0.0758, Dev loss:0.1139\n",
      "Step 100 | step loss: 0.0662\n",
      "Step 200 | step loss: 0.0749\n",
      "Step 300 | step loss: 0.0686\n",
      "Step 400 | step loss: 0.0612\n",
      "Step 500 | step loss: 0.0758\n",
      "Step 600 | step loss: 0.0617\n",
      "Step 700 | step loss: 0.0673\n",
      "Step 800 | step loss: 0.0570\n",
      "Step 900 | step loss: 0.0714\n",
      "Step 1000 | step loss: 0.0778\n",
      "Epoch 16 complete.\n",
      "Train loss:0.0679, Dev loss:0.1126\n",
      "Step 100 | step loss: 0.0653\n",
      "Step 200 | step loss: 0.0813\n",
      "Step 300 | step loss: 0.0653\n",
      "Step 400 | step loss: 0.0669\n",
      "Step 500 | step loss: 0.0694\n",
      "Step 600 | step loss: 0.0682\n",
      "Step 700 | step loss: 0.0665\n",
      "Step 800 | step loss: 0.0703\n",
      "Step 900 | step loss: 0.0607\n",
      "Step 1000 | step loss: 0.0606\n",
      "Epoch 17 complete.\n",
      "Train loss:0.0675, Dev loss:0.1061\n",
      "Step 100 | step loss: 0.0612\n",
      "Step 200 | step loss: 0.0497\n",
      "Step 300 | step loss: 0.0591\n",
      "Step 400 | step loss: 0.0793\n",
      "Step 500 | step loss: 0.0659\n",
      "Step 600 | step loss: 0.0696\n",
      "Step 700 | step loss: 0.0675\n",
      "Step 800 | step loss: 0.0714\n",
      "Step 900 | step loss: 0.0642\n",
      "Step 1000 | step loss: 0.0668\n",
      "Epoch 18 complete.\n",
      "Train loss:0.0653, Dev loss:0.1226\n",
      "Step 100 | step loss: 0.0553\n",
      "Step 200 | step loss: 0.0574\n",
      "Step 300 | step loss: 0.0800\n",
      "Step 400 | step loss: 0.0659\n",
      "Step 500 | step loss: 0.0674\n",
      "Step 600 | step loss: 0.0724\n",
      "Step 700 | step loss: 0.0673\n",
      "Step 800 | step loss: 0.0698\n",
      "Step 900 | step loss: 0.0626\n",
      "Step 1000 | step loss: 0.0713\n",
      "Epoch 19 complete.\n",
      "Train loss:0.0667, Dev loss:0.1089\n",
      "Step 100 | step loss: 0.0625\n",
      "Step 200 | step loss: 0.0723\n",
      "Step 300 | step loss: 0.0590\n",
      "Step 400 | step loss: 0.0649\n",
      "Step 500 | step loss: 0.0571\n",
      "Step 600 | step loss: 0.0688\n",
      "Step 700 | step loss: 0.0625\n",
      "Step 800 | step loss: 0.0578\n",
      "Step 900 | step loss: 0.0718\n",
      "Step 1000 | step loss: 0.0762\n",
      "Epoch 20 complete.\n",
      "Train loss:0.0656, Dev loss:0.1190\n",
      "Training complete.\n",
      "The params of best model saved.\n"
     ]
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    start = time.time()\n",
    "    \n",
    "    train_loss = train_epoch(train_dataset, model, optimizer, criterion, batch_size, device=DEVICE)\n",
    "    dev_loss = evaluate(dev_dataset, model, device=DEVICE)\n",
    "    \n",
    "    if dev_loss < best_loss:\n",
    "        best_loss = dev_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "    \n",
    "    print(f'Epoch {epoch+1} complete.\\nTrain loss:{train_loss:.4f}, Dev loss:{dev_loss:.4f}')\n",
    "\n",
    "torch.save(best_model.state_dict(), 'best_attpool_lstm.pth')\n",
    "print('Training complete.\\nThe params of best model saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "74055c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:0.1230\n"
     ]
    }
   ],
   "source": [
    "model = AttPoolLSTM(embed_dim=300, hidden_dim=400)\n",
    "model.load_state_dict(torch.load('best_attpool_lstm.pth'))\n",
    "model.to(DEVICE)\n",
    "\n",
    "test_loss = evaluate(test_dataset, model, device=DEVICE)\n",
    "print(f'Test loss:{test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4e468dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_questionwise_dataset(dataset):\n",
    "    q_dataset = dict()\n",
    "    \n",
    "    for sample in dataset:\n",
    "        qid = sample['question_id']\n",
    "        question = sample['question']\n",
    "        sentence = sample['answer']\n",
    "        label = sample['label']\n",
    "        \n",
    "        if qid in q_dataset:\n",
    "            q_dataset[qid][1].append((sentence, label))\n",
    "        else:\n",
    "            q_dataset[qid] = (question, [(sentence, label)])\n",
    "            \n",
    "    return q_dataset\n",
    "\n",
    "\n",
    "def get_scores_for_sample(sample, model, glove, device='cpu'):\n",
    "    question = sample[0]\n",
    "    question = question.translate(str.maketrans('', '', string.punctuation))\n",
    "    question = question.lower().split()\n",
    "    \n",
    "    embed_size = len(list(glove.values())[0])\n",
    "    question = [glove[q_word] if q_word in glove else np.zeros(embed_size) for q_word in question]\n",
    "    question = torch.cat([torch.Tensor(q_word).view(1, -1) for q_word in question], dim=0)\n",
    "    question = question.to(device)\n",
    "        \n",
    "    scores = list()\n",
    "    for sentence, label in sample:\n",
    "        sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "        sentence = sentence.lower().split()\n",
    "        s_vecs = [glove[s_word] if s_word in glove else np.zeros(embed_size) for s_word in sentence]\n",
    "        \n",
    "        sentence = sentence.to(device)\n",
    "        score = model(question, sentence)\n",
    "        \n",
    "        scores.append((score, label))\n",
    "        \n",
    "    scores.sort(key=lambda s: s[0], reverse=True)\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4a9e8f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "873"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwise_dataset = convert_to_questionwise_dataset(wikiqa_f['train'])\n",
    "len(qwise_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3ea148f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AttPoolLSTM' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_34736/4142851439.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_scores_for_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqwise_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_34736/486731116.py\u001b[0m in \u001b[0;36mget_scores_for_sample\u001b[0;34m(sample, model, glove, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0membed_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mglove\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq_word\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mq_word\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglove\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq_word\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq_word\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1131\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AttPoolLSTM' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "scores = get_scores_for_sample(list(qwise_dataset.values())[0], glove, best_model)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a543a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
