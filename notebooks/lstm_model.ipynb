{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "361defd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /scratch/sagarsj42/torch-cache\n",
    "!mkdir -p /scratch/sagarsj42/transformers\n",
    "!mkdir -p /scratch/sagarsj42/hf-datasets\n",
    "\n",
    "import os\n",
    "os.chdir('/scratch/sagarsj42')\n",
    "os.environ['TORCH_HOME'] = '/scratch/sagarsj42/torch-cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/scratch/sagarsj42/transformers'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/scratch/sagarsj42/hf-datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "700f0ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import string\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import datasets\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70e8c609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WikiQACorpus.zip                              100% 6928KB   6.8MB/s   00:00    \n",
      "Archive:  WikiQACorpus.zip\n",
      "  inflating: WikiQACorpus/emnlp-table/WikiQA.CNN.dev.rank  \n",
      "  inflating: WikiQACorpus/emnlp-table/WikiQA.CNN.test.rank  \n",
      "  inflating: WikiQACorpus/emnlp-table/WikiQA.CNN-Cnt.dev.rank  \n",
      "  inflating: WikiQACorpus/emnlp-table/WikiQA.CNN-Cnt.test.rank  \n",
      "  inflating: WikiQACorpus/eval.py    \n",
      "  inflating: WikiQACorpus/Guidelines_Phase1.pdf  \n",
      "  inflating: WikiQACorpus/Guidelines_Phase2.pdf  \n",
      "  inflating: WikiQACorpus/WikiQA.tsv  \n",
      "  inflating: WikiQACorpus/WikiQA-dev.ref  \n",
      "  inflating: WikiQACorpus/WikiQA-dev.tsv  \n",
      "  inflating: WikiQACorpus/WikiQA-dev.txt  \n",
      "  inflating: WikiQACorpus/WikiQA-dev-filtered.ref  \n",
      "  inflating: WikiQACorpus/WikiQASent.pos.ans.tsv  \n",
      "  inflating: WikiQACorpus/WikiQA-test.ref  \n",
      "  inflating: WikiQACorpus/WikiQA-test.tsv  \n",
      "  inflating: WikiQACorpus/WikiQA-test.txt  \n",
      "  inflating: WikiQACorpus/WikiQA-test-filtered.ref  \n",
      "  inflating: WikiQACorpus/WikiQA-train.ref  \n",
      "  inflating: WikiQACorpus/WikiQA-train.tsv  \n",
      "  inflating: WikiQACorpus/WikiQA-train.txt  \n",
      "  inflating: WikiQACorpus/LICENSE.pdf  \n",
      "  inflating: WikiQACorpus/README.txt  \n"
     ]
    }
   ],
   "source": [
    "# !scp sagarsj42@ada:/share1/sagarsj42/WikiQACorpus.zip .\n",
    "# !unzip -o WikiQACorpus.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ced921c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-29 12:30:45--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2021-11-29 12:30:46--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... ^C\n",
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    }
   ],
   "source": [
    "# !wget https://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip -o glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7adfe38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71026954",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_FILE = 'glove.6B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "518e8e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from explore_wikiqa.ipynb\n",
    "\n",
    "def get_valid_questions(wikiqa):\n",
    "    question_status = dict()\n",
    "\n",
    "    for split in wikiqa:\n",
    "        split_dataset = wikiqa[split]\n",
    "        n_samples = len(split_dataset)\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            qid = split_dataset[i]['question_id']\n",
    "            label = split_dataset[i]['label']\n",
    "            if qid not in question_status:\n",
    "                question_status[qid] = label\n",
    "            else:\n",
    "                question_status[qid] = max(question_status[qid], label)\n",
    "\n",
    "    valid_questions = set([qid for qid in question_status if question_status[qid] > 0])\n",
    "    \n",
    "    return valid_questions\n",
    "\n",
    "\n",
    "def load_glove(filename):\n",
    "    glove = dict()\n",
    "\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line_content = line.split()\n",
    "            word = line_content[0].strip()\n",
    "            vec = np.array(line_content[1:], dtype='float32')\n",
    "            glove[word] = vec\n",
    "            \n",
    "    return glove\n",
    "\n",
    "\n",
    "def get_tokens(sample):\n",
    "    question = sample['question'].translate(str.maketrans('', '', string.punctuation))\n",
    "    question = question.lower().split()\n",
    "    \n",
    "    answer = sample['answer'].translate(str.maketrans('', '', string.punctuation))\n",
    "    answer = answer.lower().split()\n",
    "    \n",
    "    return question, answer\n",
    "\n",
    "\n",
    "def get_embeddings(q_a_tokens, glove):\n",
    "    embed_size = len(list(glove.values())[0])\n",
    "    q_vecs = [glove[q_word] if q_word in glove else np.zeros(embed_size) for q_word in q_a_tokens[0]]\n",
    "    a_vecs = [glove[a_word] if a_word in glove else np.zeros(embed_size) for a_word in q_a_tokens[1]]\n",
    "    \n",
    "    return q_vecs, a_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0eed563f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset wiki_qa (/scratch/sagarsj42/hf-datasets/wiki_qa/default/0.1.0/d2d236b5cbdc6fbdab45d168b4d678a002e06ddea3525733a24558150585951c)\n",
      "Loading cached processed dataset at /scratch/sagarsj42/hf-datasets/wiki_qa/default/0.1.0/d2d236b5cbdc6fbdab45d168b4d678a002e06ddea3525733a24558150585951c/cache-dfbc6880319836b4.arrow\n",
      "Loading cached processed dataset at /scratch/sagarsj42/hf-datasets/wiki_qa/default/0.1.0/d2d236b5cbdc6fbdab45d168b4d678a002e06ddea3525733a24558150585951c/cache-f1e19b344b89867e.arrow\n",
      "Loading cached processed dataset at /scratch/sagarsj42/hf-datasets/wiki_qa/default/0.1.0/d2d236b5cbdc6fbdab45d168b4d678a002e06ddea3525733a24558150585951c/cache-5aef7c39e6a2d38e.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['question_id', 'question', 'document_title', 'answer', 'label'],\n",
       "        num_rows: 2351\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question_id', 'question', 'document_title', 'answer', 'label'],\n",
       "        num_rows: 1130\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['question_id', 'question', 'document_title', 'answer', 'label'],\n",
       "        num_rows: 8672\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikiqa = datasets.load_dataset('wiki_qa')\n",
    "valid_questions = get_valid_questions(wikiqa)\n",
    "wikiqa_f = wikiqa.filter(lambda sample: sample['question_id'] in valid_questions)\n",
    "\n",
    "wikiqa_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c6422339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove = load_glove(GLOVE_FILE)\n",
    "\n",
    "len(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "080000f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiqaDataset(Dataset):\n",
    "    def __init__(self, wikiqa, glove):\n",
    "        super(WikiqaDataset, self).__init__()\n",
    "        self.wikiqa = wikiqa\n",
    "        self.glove = glove\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.wikiqa)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.wikiqa[idx]\n",
    "        question, sentence = get_embeddings(get_tokens(sample), self.glove)\n",
    "        label = torch.tensor([sample['label']], dtype=torch.long)\n",
    "        \n",
    "        question = torch.cat([torch.Tensor(q_word).view(1, -1) for q_word in question], dim=0)\n",
    "        sentence = torch.cat([torch.Tensor(s_word).view(1, -1) for s_word in sentence], dim=0)\n",
    "        \n",
    "        return question, sentence, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "677aa965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dual_att_projection',\n",
       "  Parameter containing:\n",
       "  tensor([[0.3619, 0.1466, 0.2737,  ..., 0.7473, 0.2578, 0.8925],\n",
       "          [0.1477, 0.9535, 0.9919,  ..., 0.1017, 0.9293, 0.1048],\n",
       "          [0.8272, 0.3024, 0.3761,  ..., 0.5615, 0.9536, 0.4489],\n",
       "          ...,\n",
       "          [0.5078, 0.9481, 0.3531,  ..., 0.4432, 0.2870, 0.0515],\n",
       "          [0.9998, 0.7741, 0.5814,  ..., 0.8803, 0.3475, 0.0534],\n",
       "          [0.6585, 0.4045, 0.5212,  ..., 0.8566, 0.2748, 0.7356]],\n",
       "         requires_grad=True)),\n",
       " ('projection.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0235,  0.0055, -0.0002,  ...,  0.0337, -0.0019, -0.0013],\n",
       "          [ 0.0347, -0.0337,  0.0566,  ..., -0.0352, -0.0195,  0.0127],\n",
       "          [ 0.0450,  0.0121, -0.0424,  ...,  0.0342, -0.0419,  0.0478],\n",
       "          ...,\n",
       "          [-0.0345,  0.0021,  0.0122,  ...,  0.0164, -0.0144, -0.0035],\n",
       "          [-0.0457,  0.0473, -0.0239,  ..., -0.0410, -0.0219, -0.0072],\n",
       "          [-0.0238, -0.0521,  0.0518,  ..., -0.0030,  0.0164, -0.0180]],\n",
       "         requires_grad=True)),\n",
       " ('projection.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 4.4653e-02, -2.1492e-02,  5.3259e-02, -3.3430e-02,  7.8154e-03,\n",
       "           5.0757e-02,  3.0469e-02, -5.3751e-02,  3.5666e-02,  1.2945e-02,\n",
       "           3.6839e-02,  2.2505e-02,  2.9143e-02, -1.2085e-02, -4.5381e-02,\n",
       "          -1.4974e-02,  4.9677e-02,  1.8133e-02, -4.9529e-02,  2.7874e-02,\n",
       "          -3.2614e-02, -5.2174e-04,  9.8331e-03, -1.4903e-02,  3.5443e-03,\n",
       "           4.4719e-02, -4.0301e-02,  4.9970e-02,  5.3369e-02,  2.2940e-03,\n",
       "           3.5846e-02,  3.0723e-02,  3.8535e-02,  4.4622e-02, -1.4050e-02,\n",
       "           3.6747e-02,  1.9127e-02, -5.5294e-02,  2.1609e-02,  5.1921e-03,\n",
       "          -3.5078e-02,  2.0216e-02,  1.7551e-02,  2.8093e-02,  3.5389e-02,\n",
       "          -1.2208e-02,  1.6561e-02,  2.1644e-02,  3.2624e-02,  5.6132e-02,\n",
       "          -2.6148e-02, -4.1987e-02, -1.0911e-02, -1.4821e-02, -5.5151e-03,\n",
       "          -9.3438e-03, -4.5152e-02, -3.0210e-02, -8.6673e-03, -1.0541e-02,\n",
       "          -4.7578e-02, -1.6623e-02, -2.5763e-02, -1.2756e-03,  3.0380e-02,\n",
       "          -2.8536e-02,  3.5582e-02,  2.1353e-02, -1.2619e-02, -5.4759e-02,\n",
       "           3.2035e-02,  6.2097e-03, -5.0393e-02,  5.1506e-02,  9.4656e-03,\n",
       "           1.2383e-02,  3.6431e-02, -3.9942e-02,  3.6650e-02,  9.4678e-03,\n",
       "           4.3260e-02, -5.7416e-02, -4.7941e-02,  2.5258e-02, -4.7993e-02,\n",
       "           2.0716e-02, -3.8448e-02,  5.4649e-02, -2.1641e-02, -3.3748e-02,\n",
       "           5.3387e-02,  2.4164e-02,  4.5740e-02, -8.9027e-03, -1.6391e-02,\n",
       "           3.8861e-03,  2.0551e-02, -3.4369e-02, -4.1792e-02, -1.9439e-03,\n",
       "           7.1928e-03,  4.2715e-02,  2.3975e-02, -2.1585e-02,  2.4690e-02,\n",
       "           6.7868e-03,  3.5819e-02, -8.7829e-03, -4.5190e-02, -5.1139e-02,\n",
       "           1.1738e-02,  1.8445e-02,  1.4996e-02,  4.1265e-02,  3.3073e-03,\n",
       "           4.4456e-03,  5.1137e-02, -3.2625e-02, -2.3057e-02,  2.2604e-02,\n",
       "          -6.7190e-04, -4.1303e-02, -1.3659e-02,  1.1653e-02, -2.4835e-03,\n",
       "          -4.7833e-02, -9.3166e-03, -4.5244e-02, -1.9368e-02, -1.1174e-02,\n",
       "           2.6872e-02, -5.9207e-03, -1.1083e-02,  1.2737e-02, -2.6178e-02,\n",
       "          -3.4461e-02, -2.3085e-02,  4.9254e-02, -3.7494e-02,  3.6601e-02,\n",
       "           5.1436e-02,  5.2514e-02,  4.4457e-02, -1.7970e-02,  4.3519e-02,\n",
       "           4.0687e-02, -4.0465e-02,  3.6401e-03, -4.0568e-02,  4.6085e-02,\n",
       "          -3.6244e-02, -4.8630e-02,  1.2641e-02,  5.1006e-02, -5.2984e-02,\n",
       "           4.6606e-02, -5.3469e-02, -1.0896e-02,  4.6563e-02,  1.4063e-02,\n",
       "           3.8751e-02, -2.4270e-02,  1.0766e-02,  3.3802e-02, -4.4989e-03,\n",
       "           2.9832e-02, -5.0404e-03,  4.8960e-02, -3.6704e-02,  2.0471e-02,\n",
       "           5.1321e-03,  5.6855e-02, -1.3388e-02,  1.9361e-02,  4.8648e-02,\n",
       "           5.0750e-02, -8.4588e-03,  9.5832e-03, -9.5096e-03, -1.9836e-02,\n",
       "           5.4693e-02,  1.3985e-02,  5.0849e-02,  2.6812e-02,  4.7298e-02,\n",
       "          -4.4006e-02, -1.3968e-02,  2.6129e-02, -5.2938e-02, -5.6131e-02,\n",
       "           5.4949e-02, -1.7624e-02,  1.4521e-02, -4.9053e-02,  3.3410e-02,\n",
       "           3.0637e-02, -5.7378e-02,  1.7805e-02,  2.6126e-05, -8.7781e-03,\n",
       "          -5.5796e-02,  3.2049e-02, -3.7160e-02,  1.7660e-02, -2.1196e-02,\n",
       "           5.2974e-02, -5.1547e-02, -4.4498e-02, -1.4084e-02,  2.1332e-02,\n",
       "          -5.4404e-03, -5.6799e-02, -9.9749e-03, -1.9123e-02, -2.4041e-02,\n",
       "          -2.6253e-02, -4.0680e-02,  5.5242e-02, -3.4083e-02,  1.5632e-02,\n",
       "           4.8690e-02,  5.5347e-02,  4.4036e-02,  1.7841e-02, -4.0853e-02,\n",
       "          -3.7464e-02,  4.1589e-02,  2.9760e-02,  4.1463e-03, -4.4305e-02,\n",
       "          -2.1451e-02, -3.6486e-02,  5.0853e-03,  1.4620e-02, -4.3428e-02,\n",
       "          -3.8914e-03, -3.7049e-03, -8.8343e-03, -1.5800e-02,  3.3089e-02,\n",
       "           5.6532e-02,  4.3741e-02, -4.6547e-02,  2.1153e-02,  3.2723e-02,\n",
       "           2.7221e-02,  3.2123e-02,  3.6820e-02,  5.7497e-02,  1.0315e-02,\n",
       "           2.1422e-02, -4.0634e-02,  1.2410e-02, -1.8220e-02, -3.9948e-02,\n",
       "          -2.1917e-02,  3.7930e-02,  2.4789e-02, -2.5049e-02,  5.3752e-02,\n",
       "           5.5305e-02,  5.0576e-02,  2.7865e-02,  2.3134e-02,  4.0189e-02,\n",
       "           1.1545e-02, -1.1223e-02,  3.5375e-02, -5.5738e-02, -3.6473e-02,\n",
       "           3.6097e-02,  3.5969e-02, -3.0133e-02,  2.3836e-02,  3.1760e-02,\n",
       "          -5.6033e-02,  9.4828e-03,  8.9306e-03,  4.2780e-02, -4.7611e-02,\n",
       "           1.8154e-02,  3.8204e-02, -2.8314e-03,  3.6807e-02, -3.8797e-02,\n",
       "          -4.4976e-02, -2.4508e-02,  5.5273e-04,  5.3352e-02, -4.0827e-02,\n",
       "          -3.4595e-02,  3.4741e-02,  3.0083e-02,  9.1236e-03,  4.4722e-02,\n",
       "           4.2496e-02,  3.7193e-02, -3.6437e-02, -2.4018e-02,  5.0473e-02],\n",
       "         requires_grad=True)),\n",
       " ('lstm.weight_ih_l0',\n",
       "  Parameter containing:\n",
       "  tensor([[-3.6823e-02,  3.7587e-02,  3.3268e-02,  ..., -1.2288e-02,\n",
       "            5.5276e-02, -1.4821e-02],\n",
       "          [ 4.6102e-02, -3.3833e-02,  5.5148e-02,  ...,  3.9059e-02,\n",
       "           -5.1492e-02,  2.8131e-02],\n",
       "          [ 1.7133e-04, -3.0059e-03, -1.1854e-02,  ..., -2.7129e-02,\n",
       "            2.9580e-02,  2.4026e-02],\n",
       "          ...,\n",
       "          [ 3.2759e-02,  2.7980e-02,  8.1166e-05,  ...,  2.5361e-02,\n",
       "            6.1441e-03, -2.0272e-02],\n",
       "          [-4.3258e-02,  5.0561e-02, -3.0010e-02,  ...,  5.5277e-02,\n",
       "           -2.4181e-02, -2.8159e-03],\n",
       "          [ 2.2690e-02, -2.6207e-02,  2.0523e-02,  ...,  1.1908e-02,\n",
       "            4.1616e-02,  3.8632e-02]], requires_grad=True)),\n",
       " ('lstm.weight_hh_l0',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0488, -0.0215, -0.0029,  ..., -0.0226,  0.0426,  0.0253],\n",
       "          [ 0.0011,  0.0284,  0.0086,  ..., -0.0200,  0.0097,  0.0528],\n",
       "          [-0.0338, -0.0117, -0.0297,  ..., -0.0304,  0.0206,  0.0108],\n",
       "          ...,\n",
       "          [ 0.0400, -0.0278,  0.0110,  ..., -0.0167,  0.0272,  0.0357],\n",
       "          [ 0.0499, -0.0386, -0.0526,  ..., -0.0482,  0.0205,  0.0244],\n",
       "          [-0.0135,  0.0037, -0.0513,  ..., -0.0447, -0.0221,  0.0323]],\n",
       "         requires_grad=True)),\n",
       " ('lstm.bias_ih_l0',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0163,  0.0188,  0.0240,  ..., -0.0093,  0.0166, -0.0308],\n",
       "         requires_grad=True)),\n",
       " ('lstm.bias_hh_l0',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0164,  0.0069,  0.0054,  ..., -0.0383,  0.0548, -0.0479],\n",
       "         requires_grad=True)),\n",
       " ('lstm.weight_ih_l0_reverse',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0319, -0.0231, -0.0021,  ..., -0.0254,  0.0156, -0.0341],\n",
       "          [ 0.0072, -0.0338, -0.0506,  ..., -0.0117,  0.0104,  0.0177],\n",
       "          [ 0.0266,  0.0482, -0.0561,  ..., -0.0280,  0.0361,  0.0450],\n",
       "          ...,\n",
       "          [-0.0440,  0.0147,  0.0132,  ..., -0.0109, -0.0164, -0.0282],\n",
       "          [ 0.0037,  0.0223,  0.0244,  ...,  0.0151,  0.0369, -0.0371],\n",
       "          [ 0.0084, -0.0516, -0.0438,  ..., -0.0366, -0.0302,  0.0110]],\n",
       "         requires_grad=True)),\n",
       " ('lstm.weight_hh_l0_reverse',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0050, -0.0333, -0.0308,  ...,  0.0384, -0.0113, -0.0449],\n",
       "          [ 0.0116, -0.0337, -0.0050,  ..., -0.0456, -0.0137,  0.0241],\n",
       "          [-0.0454, -0.0473,  0.0229,  ...,  0.0523, -0.0346, -0.0162],\n",
       "          ...,\n",
       "          [ 0.0409, -0.0376, -0.0353,  ...,  0.0515, -0.0385,  0.0133],\n",
       "          [ 0.0479,  0.0009, -0.0136,  ..., -0.0015, -0.0525,  0.0466],\n",
       "          [ 0.0246,  0.0452,  0.0014,  ...,  0.0437, -0.0017,  0.0451]],\n",
       "         requires_grad=True)),\n",
       " ('lstm.bias_ih_l0_reverse',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0302, -0.0552,  0.0167,  ...,  0.0001, -0.0220,  0.0477],\n",
       "         requires_grad=True)),\n",
       " ('lstm.bias_hh_l0_reverse',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0559, -0.0371,  0.0109,  ...,  0.0435,  0.0284,  0.0059],\n",
       "         requires_grad=True))]"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AttPoolLSTM(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, bidirectional=True):\n",
    "        super(AttPoolLSTM, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.projection = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=self.embed_dim, hidden_size=self.hidden_dim, num_layers=1, \n",
    "                            batch_first=True, bidirectional=bidirectional)\n",
    "        \n",
    "        self.dual_att_projection = nn.Parameter(torch.rand(2*self.hidden_dim, 2*self.hidden_dim))\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        self.cosine = nn.CosineSimilarity(dim=-1)\n",
    "        \n",
    "        \n",
    "    def forward(self, question, sentence):\n",
    "        if len(question.shape) == 2:\n",
    "            question = question.unsqueeze(dim=0)\n",
    "            sentence = sentence.unsqueeze(dim=0)\n",
    "        \n",
    "        question = self.projection(question)\n",
    "        question, _ = self.lstm(question)\n",
    "        sentence = self.projection(sentence)\n",
    "        sentence, _ = self.lstm(sentence)\n",
    "        \n",
    "        n = sentence.shape[0]\n",
    "        l = sentence.shape[1]\n",
    "        c = sentence.shape[2]\n",
    "        qs_alignment = torch.matmul(torch.matmul(question, self.dual_att_projection), sentence.view(n, c, l))\n",
    "        \n",
    "        q_pool = torch.max(qs_alignment, dim=2)[0]\n",
    "        q_pool = self.softmax(q_pool)\n",
    "        s_pool = torch.max(qs_alignment, dim=1)[0]\n",
    "        s_pool = self.softmax(s_pool)\n",
    "        \n",
    "        q_rep = torch.matmul(question.transpose(1, 2), q_pool.transpose(0, 1)).transpose(1, 2)\n",
    "        s_rep = torch.matmul(sentence.transpose(1, 2), s_pool.transpose(0, 1)).transpose(1, 2)\n",
    "        \n",
    "        match_score = self.cosine(q_rep, s_rep)\n",
    "        \n",
    "        return match_score\n",
    "    \n",
    "\n",
    "model = AttPoolLSTM(300, 300)\n",
    "list(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "e9758abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6, 300]), torch.Size([1, 35, 300]))"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question, sentence = get_embeddings(get_tokens(wikiqa_f['train'][100]), glove)\n",
    "question = torch.cat([torch.Tensor(q_word).view(1, -1) for q_word in question], dim=0).unsqueeze(0)\n",
    "sentence = torch.cat([torch.Tensor(s_word).view(1, -1) for s_word in sentence], dim=0).unsqueeze(0)\n",
    "\n",
    "question.shape, sentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "4c332ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7626]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_score = model(question, sentence)\n",
    "match_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "6eab7d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttPoolLSTM(\n",
       "  (projection): Linear(in_features=300, out_features=300, bias=True)\n",
       "  (lstm): LSTM(300, 400, batch_first=True, bidirectional=True)\n",
       "  (softmax): Softmax(dim=-1)\n",
       "  (cosine): CosineSimilarity()\n",
       ")"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AttPoolLSTM(embed_dim=300, hidden_dim=400)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "3c442923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8672, 1130, 2351)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = WikiqaDataset(wikiqa_f['train'], glove)\n",
    "dev_dataset = WikiqaDataset(wikiqa_f['validation'], glove)\n",
    "test_dataset = WikiqaDataset(wikiqa_f['test'], glove)\n",
    "\n",
    "len(train_dataset), len(dev_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "ad3ad9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarginRankingLoss()"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 4\n",
    "n_epochs = 20\n",
    "\n",
    "criterion = nn.MarginRankingLoss(margin=0.1)\n",
    "criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "1e09382f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0.0\n",
       "    lr: 0.001\n",
       "    momentum: 0.6\n",
       "    nesterov: True\n",
       "    weight_decay: 0.001\n",
       ")"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.6, weight_decay=0.001, dampening=0.0, nesterov=True)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "4d0e0219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_candidate(dataset, indices, model, select_from=20, device='cpu'):\n",
    "    candidates = random.sample(indices, select_from)\n",
    "    candidate_scores = list()\n",
    "    for c_i in candidates:\n",
    "        c_q, c_s, c_l = dataset[c_i]\n",
    "        c_q = c_q.to(device)\n",
    "        c_s = c_s.to(device)\n",
    "        \n",
    "        if c_l == 0:\n",
    "            cand_match_score = model(c_q, c_s)\n",
    "            candidate_scores.append(cand_match_score.item())\n",
    "    if len(candidate_scores) > 0:\n",
    "        neg_sample_idx = candidates[np.argmax(np.array(candidate_scores))]\n",
    "    else:\n",
    "        neg_sample_idx = 0\n",
    "        \n",
    "    neg_sample = dataset[neg_sample_idx]\n",
    "    n_question, n_sentence, _ = neg_sample\n",
    "    \n",
    "    return n_question.to(device), n_sentence.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "8852d8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataset, model, optimizer, criterion, batch_size, device='cpu'):\n",
    "    model.train()\n",
    "    indices = list(range(len(dataset)))\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    batch_loss = 0.0\n",
    "    batch_count = 0\n",
    "    step_count = 0\n",
    "    \n",
    "    for i in indices:\n",
    "        question, sentence, label = dataset[i]\n",
    "        if label == 1:\n",
    "            question = question.to(device)\n",
    "            sentence = sentence.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pos_match_score = model(question, sentence)\n",
    "            n_question, n_sentence = find_best_candidate(dataset, indices, model, \n",
    "                                                         select_from=20, device=device)\n",
    "            neg_match_score = model(n_question, n_sentence)\n",
    "            loss = criterion(pos_match_score, neg_match_score, label)\n",
    "            batch_loss += loss\n",
    "            batch_count += 1\n",
    "            \n",
    "            if batch_count % batch_size == 0:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                batch_loss = 0.0\n",
    "                batch_count = 0\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            step_count += 1\n",
    "            \n",
    "            if step_count % 100 == 0:\n",
    "                print(step_count, 'steps done')\n",
    "            \n",
    "    return total_loss / step_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "7fdfea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataset, model, device='cpu'):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    step_count = 0\n",
    "    indices = list(range(len(dataset)))\n",
    "    \n",
    "    for i in indices:\n",
    "        question, sentence, label = dataset[i]\n",
    "        if label == 1:\n",
    "            question = question.to(device)\n",
    "            sentence = sentence.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pos_match_score = model(question, sentence)\n",
    "            n_question, n_sentence = find_best_candidate(dataset, indices, model, \n",
    "                                                         select_from=20, device=device)\n",
    "            neg_match_score = model(n_question, n_sentence)\n",
    "            loss = criterion(pos_match_score, neg_match_score, label)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            step_count += 1\n",
    "    \n",
    "    return total_loss / step_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "95fac3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 complete.\n",
      "Train loss:0.1319, Dev loss:0.1394\n",
      "Epoch 1 complete.\n",
      "Train loss:0.1319, Dev loss:0.1551\n",
      "Epoch 2 complete.\n",
      "Train loss:0.1319, Dev loss:0.1422\n",
      "Epoch 3 complete.\n",
      "Train loss:0.1319, Dev loss:0.1567\n",
      "Epoch 4 complete.\n",
      "Train loss:0.1319, Dev loss:0.1416\n",
      "Epoch 5 complete.\n",
      "Train loss:0.1319, Dev loss:0.1421\n",
      "Epoch 6 complete.\n",
      "Train loss:0.1319, Dev loss:0.1519\n",
      "Epoch 7 complete.\n",
      "Train loss:0.1319, Dev loss:0.1491\n",
      "Epoch 8 complete.\n",
      "Train loss:0.1319, Dev loss:0.1491\n",
      "Epoch 9 complete.\n",
      "Train loss:0.1319, Dev loss:0.1482\n",
      "Epoch 10 complete.\n",
      "Train loss:0.1319, Dev loss:0.1402\n",
      "Epoch 11 complete.\n",
      "Train loss:0.1319, Dev loss:0.1492\n",
      "Epoch 12 complete.\n",
      "Train loss:0.1319, Dev loss:0.1459\n",
      "Epoch 13 complete.\n",
      "Train loss:0.1319, Dev loss:0.1502\n",
      "Epoch 14 complete.\n",
      "Train loss:0.1319, Dev loss:0.1503\n",
      "Epoch 15 complete.\n",
      "Train loss:0.1319, Dev loss:0.1413\n",
      "Epoch 16 complete.\n",
      "Train loss:0.1319, Dev loss:0.1566\n",
      "Epoch 17 complete.\n",
      "Train loss:0.1319, Dev loss:0.1533\n",
      "Epoch 18 complete.\n",
      "Train loss:0.1319, Dev loss:0.1432\n",
      "Epoch 19 complete.\n",
      "Train loss:0.1319, Dev loss:0.1463\n",
      "Training complete.\n",
      "The params of best model saved.\n"
     ]
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    start = time.time()\n",
    "    \n",
    "    train_loss = train_epoch(train_dataset, model, optimizer, criterion, batch_size, device=DEVICE)\n",
    "    dev_loss = evaluate(dev_dataset, model, device=DEVICE)\n",
    "    \n",
    "    if dev_loss < best_loss:\n",
    "        best_loss = dev_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "    \n",
    "    print(f'Epoch {epoch} complete.\\nTrain loss:{train_loss:.4f}, Dev loss:{dev_loss:.4f}')\n",
    "\n",
    "torch.save(best_model.state_dict(), 'best_attpool_lstm.pth')\n",
    "print('Training complete.\\nThe params of best model saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74055c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttPoolLSTM(embed_dim=300, hidden_dim=400)\n",
    "model.load_state_dict(torch.load('best_attpool_lstm.pth'))\n",
    "\n",
    "test_loss = evaluate(test_dataset, model, device=DEVICE)\n",
    "print(f'Test loss:{test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "4e468dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_questionwise_dataset(dataset):\n",
    "    q_dataset = dict()\n",
    "    \n",
    "    for sample in dataset:\n",
    "        qid = sample['question_id']\n",
    "        question = sample['question']\n",
    "        sentence = sample['answer']\n",
    "        label = sample['label']\n",
    "        \n",
    "        if qid in q_dataset:\n",
    "            q_dataset[qid][1].append((sentence, label))\n",
    "        else:\n",
    "            q_dataset[qid] = (question, [(sentence, label)])\n",
    "            \n",
    "    return q_dataset\n",
    "\n",
    "\n",
    "def get_scores_for_sample(sample, model, glove, device='cpu'):\n",
    "    question = sample[0]\n",
    "    question = question.translate(str.maketrans('', '', string.punctuation))\n",
    "    question = question.lower().split()\n",
    "    \n",
    "    embed_size = len(list(glove.values())[0])\n",
    "    question = [glove[q_word] if q_word in glove else np.zeros(embed_size) for q_word in question]\n",
    "    question = torch.cat([torch.Tensor(q_word).view(1, -1) for q_word in question], dim=0)\n",
    "    question = question.to(device)\n",
    "        \n",
    "    scores = list()\n",
    "    for sentence, label in sample:\n",
    "        sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "        sentence = sentence.lower().split()\n",
    "        s_vecs = [glove[s_word] if s_word in glove else np.zeros(embed_size) for s_word in sentence]\n",
    "        \n",
    "        sentence = sentence.to(device)\n",
    "        score = model(question, sentence)\n",
    "        \n",
    "        scores.append((score, label))\n",
    "        \n",
    "    scores.sort(key=lambda s: s[0], reverse=True)\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "4a9e8f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "873"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwise_dataset = convert_to_questionwise_dataset(wikiqa_f['train'])\n",
    "len(qwise_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "3ea148f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1203/4142851439.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_scores_for_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqwise_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1203/3788032773.py\u001b[0m in \u001b[0;36mget_scores_for_sample\u001b[0;34m(sample, model, glove, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1203/3876873268.py\u001b[0m in \u001b[0;36mget_tokens\u001b[0;34m(sample)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "scores = get_scores_for_sample(list(qwise_dataset.values())[0], glove, best_model)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a543a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
